The escalating growth of electronic information in the field of healthcare puts a spotlight on retrospective clinical trials. Processing large amounts of data with traditional statistical methods is not always effective.

%or it is sometimes limited by implementation constraints.

Due to the unique nature of healthcare and the complexity of the human biological system, for efficient analysis, data mining methods can only be used after area-specific extensions. Improved and healthcare-adapted data science methods can effectively contribute to the analysis of retrospective clinical trials, and they can provide a basis to explore the influencing factors affecting the human biology system. This new knowledge can help physicians achieve individualised medicine.

The aim of my research was to develop such new healthcare-adapted data science methods and algorithms that can effectively contribute to the exploration of information from large, sometimes unstructured healthcare datasets, and extract useful information from them.

My research included the following topics: development of new control group selection methods for retrospective case-control studies; developing new similarity measures for evaluating the results of the control group selection; analysing the effect of missing variables during the control group selection process; and extracting information from large, unstructured healthcare datasets.

\section{Control group selection}
\label{sec:cgs_intro}

Comparative analysis methods are widely used in observational studies in the field of social sciences \cite{rasoolimanesh2023guideline}, natural sciences \cite{zeng2023comparative} and engineering \cite{abualigah2022meta}. Although these comparison-based scientific analyses significantly differ in the applied methodology and study design principles \cite{song2010observational, wacholder1992selection3}, due to their comparative nature they have strong scientific evidence \cite{horton2011encyclopaedic, zschoch2011configurational, pickvance2001four, wacholder1992selection,wacholder1992selection2}.

In human comparative cohort studies, people are classified into two independent groups (cohorts) \cite{wacholder1992selection}, namely into a case group and a control group. The selection of these groups is very important and has a significant impact on the output of the analysis as well. Individuals of these groups have to be similar in many ways (e.g., gender and age distribution), but they have to differ in an examined characteristic property (e.g.,patients in the case group are treated with a certain medicine, while individuals in the control group receive placebo) \cite{wacholder1992selection, wacholder1992selection2, iwagami2022introduction}.

In prospective study design \cite{wacholder1992selection3, iwagami2022introduction}, there are many inclusion and exclusion criteria specified to select the proper individuals into the case and the control groups. Patient-specific data, thought to be important and relevant, are systematically collected and recorded during the whole study period. The main disadvantage of these studies is that the execution of a study sometimes takes up a lot of time.

In contrast, retrospective cohort studies \cite{wacholder1992selection3, iwagami2022introduction} look back in the time and they do not require a long time for collecting data about patients. However, these studies must face the fact that the range of available data is not always complete. The popularity of case-control studies, especially retrospective case-control studies, arises from their relatively inexpensive nature, however, the degree of their evidence is lower than that of randomised trials.

A prerequisite of carrying out appropriate analyses is that the case and control groups have to be similar on covariates (independent variables) that predict group membership (treatment assignment) and affect the examined output. However, fulfilling these requirements is not a trivial task. Many articles have highlighted the importance of the proper implementation of a control group selection method and the effect of unbalanced control groups on the result of analyses \cite{harris2002control, pell2008selection, behar2008effect, ripollone2018implications, moser2019out}. The selection of the case group can be carried out based on the study aims, but the determination of the control group may have difficulties and raises many questions \cite{wacholder1992selection, koepsell2014epidemiologic}. The reliability of these studies can be improved by (1) increasing the number of cases included in the study, (2) performing thorough data preparation and data cleaning activities, and (3) selecting a proper control group for the case group.

In the literature, various methods have been proposed for selecting a control group. Some of them use different sampling methods (e.g.,simple randomised sampling or stratified sampling \cite{jewell1985least, singh2003stratified}), while others are based on propensity score matching (PSM) \cite{rosenbaum1983central, austin2011introduction}. Nowadays, PSM \cite{rosenbaum1983central} is the most widely used control group selection method. It is widespread in healthcare analyses \cite{shin2015comparative, tokuda2019clinical, chuang2019acute}, and is gaining ground in social sciences \cite{thoemmes2011systematic, hwang2018rethinking, xu2020examining} and economics \cite{shipman2017propensity, peel2009propensity, cushman2017exchange, rosholm2019bridging}.

PSM matches the individuals of the case and control groups based on their propensity score values, which is the probability of the group (treatment) assignment conditional on the observed baseline covariates. Over recent decades, different PSM methods have been proposed (e.g.,radius matching, nearest-neighbour matching, stratified matching, kernel matching, Mahalanobis distance matching \cite{baser2006too, austin2011introduction, caliendo2008some}) to reduce the imbalance of the confounders between the case and control groups \cite{stuart2010matching}. Despite the popularity of these methods, they have also received much criticism \cite{biondi2011propensity, pearl2009remarks, mansournia2018case}.

%For more details, see Section \ref{sec:psm}.

The main limitation of the PSM methods is that they map the feature space into a single value (propensity score), and the matching of the individuals is performed in this compressed space. This can cause the problem of competing risks, which was also highlighted in \cite{he2021optimal}.

Publications \cite{stuart2010matching} and \cite{wan2019matched} also highlighted that matched-pair analysis has to be performed only when matched individuals are highly correlated, but matching subjects having similar propensity scores does not necessarily result in matched subjects with similar covariate values. Paul Moser in a recently published book wrote that during control group selection, we try to control the influence of the known knowns and the known unknowns \cite{moser2019out}. Therefore, if we convert the known things into a compressed 1-dimensional space, which is not able to express as much information as in the original, more informative high-dimensional vector space of the features, the effect of the known covariates cannot be controlled to such a degree.

The selection of covariates is a critical step in case-control studies, and the results of case-control studies rest on a correctly constructed dataset and adequate control group selection. By these considerations, I regard matching in the original $n$-dimensional vector space or its subspace more suitable than in the 1-dimensional space of propensity scores. The mentioned subspace refers to the covariates which should be included in the propensity score model. Austin \cite{austin2007comparison} and Brookhart \cite{brookhart2006variable} recommend that all variables that affect both the exposure of the treatment (group membership) and the outcome of the study should always be taken into account.

For the aforementioned purposes, I developed a novel nearest neighbour-based control group selection method called Weighted Nearest Neighbours Control Group Selection with Error Minimization (WNNEM). The WNNEM method can be seen as a hybrid combination of the PSM method and the nearest neighbour principle, as matching is performed based on the nearest neighbours, but the distances are weighted according to the relevance of the covariates. In \cite{szeker2020weighted}, I have presented that the WNNEM method can select more balanced control groups than the greedy PSM method, especially in cases when individuals are characterised only by few covariates and covariates can take only a few values. However, the WNNEM method presented in \cite{szeker2020weighted} also has some limitations. On the one hand, it can not handle covariates negatively associated to the treatment assignment; on the other hand, the method can be further improved by utilising probabilistic optimisation for hand-ling more complex problems. Therefore, I proposed a novel nearest neighbour-based control group selection algorithm called Weighted Nearest Neighbour Control Group Selection with Simulated Annealing (WNNSA) \cite{szeker2021optimized}, which uses simulated annealing for finding the best pairing of the individuals. The proposed algorithm can handle both positive and negative covariates concerning the effect on the probability of the treatment assignment.

The usability of a control group selection method can be defined as how similar the selected control group is to the case group, as the degree of similarity has a significant impact on the evaluation of test results. The evaluation can happen by measuring the similarity of paired individuals from the case and control groups (paired evaluation) or by assessing the similarity of descriptive covariates of the case and control groups (non-paired evaluation). However, the similarity of the covariates of these cohorts is generally not expressed as a single quantitative measure. Only the applied control group selection methods suggest some recommendations on how to perform them in order to be able to select an adequate control group from the available population.

Most of the applied non-paired evaluation methods are Goodness of Fit (GoF) tests (e.g.,Kolgomorov-Smirnov test, Bhattacharyya distance, Matusita distance) \cite{mielke2007permutation, berry2011permutation} evaluating the distribution of the two groups. Using a GoF test, it is possible to evaluate a 1-dimensional distribution (that is the similarity of a certain property), but it is nearly impossible for higher dimensions \cite{fasano1987multidimensional}.

However, people as the elements of the groups are characterised not by one but by many features. On the other hand, if the elements of the control group are selected by propensity score matching, the similarity of the case and control elements is measured again only in one dimension, namely as the dissimilarities of the propensity scores. As the propensity score is an estimated value, the similarity measurement is made in a lossy compressed 1-dimensional space, and not in the original feature space of the elements.

Contrary to these methods, my aim was to measure the similarity of the case and control groups in the original high-dimensional feature space of the individuals. For this reason, I proposed three quantitative dissimilarity measures to measure the dissimilarity of the case group and the control group in combination with the previously introduced methods \cite{szeker2018measuring, szeker2019can}. Two of them evaluate the similarities of case and control groups based on the similarities of the paired individuals and the third one compares the distribution of the characteristic features of the groups. The versatility of the proposed methods was shown on synthetic datasets. Results point out the fact that it is worth considering the proposed measures together to evaluate the similarity of case and control groups and allow researchers to express the degree of similarity of two cohorts quantitatively.

Furthermore, I also analysed the effect of missing dichotomous variables on the deviation of the outcome variable. The analysis was based on a Monte Carlo simulations in which I modelled the effect of omitted variables on the outcome. To measure the bias of the outcome I applied logistic regression-based Propensity Score Matching. I established that in the pessimistic scenario the omitted variables with high significance could greatly affect the value of the outcome variable. This conclusion is based on the revealed linear relationship between the deviation of the outcome and the model accuracy \cite{szeker2018effect}. This analysis drew attention to the important fact that calculations with missing variables can significantly influence the evaluation of case-control studies.

\section{Information extraction from echocardiography documents}
\label{sec:tm_intro}

Hospital information systems (HIS) are widely applied information systems for collecting, storing, and managing electronic medical records (EMR). Besides their constantly expanding functionality (e.g.,collecting biosensor data), the analysis of information stored in HISs also becomes increasingly important. EMRs are valuable information sources for medical analysis, however they are usually incomplete or redundant, making data mining a difficult and challenging task. The efficiency of information extraction and processing from stored data is significantly influenced by the primary form of data recording. Nowadays, hospital information systems store a large amount of data in a structured form (e.g.,personal data, laboratory results), but there are still findings recorded in semi-structured and free-text written format (e.g.,anamnesis, echocardiography results). Although the exploitation of information in the data is still typically achieved by human intelligence, artificial intelligence (AI) algorithms are also gaining ground in this area and help healthcare professionals in solving several domain-specific tasks.

Generally, information extraction from medical texts focuses on the following two tasks: named-entity recognition (NER, or term extraction) and relation extraction (RE). Named-entity recognition refers to the process of identifying particular types of names, terminologies or symbols in documents, while relation extraction identifies the relation between them \cite{sun2018data}.

Successful term identification is key to getting access to the stored information and the process of identification has been recognised as a bottleneck in text mining. The process of term identification is usually done in three steps: the first step is term recognition; the second step is term classification; and the last step is term mapping \cite{krauthammer2004term}.

There are two possible approaches to identify terms. The first approach is to directly search for specific terms (e.g.,aortic root, ejection fraction) in documents. Direct search can also be extended by pattern search, which requires a priori knowledge about the structure of the processed text (e.g.,use of colon between terms and values, order of terms, various expletives). With this extension, it becomes possible to recognise terms and their measured value (e.g.,aortic root: 27 mm) together. Other term extraction methods also exist which utilise classical text mining techniques. These text mining-based solutions simply collect every occurrence of word sequences that are possibly valid terms. However, these methods require a text pre-processing phase (including text cleaning), and term candidates must be identified and mapped onto a dictionary after term extraction.

% Direct term search always relies on a specialised dictionary to recognise and classify medical terminology, and the performance of this approach heavily depends on the coverage and quality of the dictionary. The acquisition of such knowledge is a time-consuming task.

% It is especially true in case of echocardiography reports.

In the literature, several studies have been published which are engaged in echocardiography report processing \cite{xie2017extracting, garvin2012automated, kim2017extraction, patterson2017unlocking, wells2014extraction, toepfer2015fine, jonnalagadda2017text, renganathan2017text}. Generally, echocardiography reports can be divided into two parts in terms of diagnostic content: in the first semi-structured part diagnostic results are stored in the form of term-value pairs (e.g., interventricular septum: 14 mm) and in the second part results are recorded as free text written in natural language (e.g., mild left ventricular hypertrophy). Processing echocardiography reports is a nontrivial task as the storage of echocardiography examinations varies across different medical institutes.

The methods proposed in the literature are mostly based on the direct search approach, but some of them apply text mining methods as well. In the published studies, typically only the extraction of one specific parameter is the aim, such as ejection fraction (EF). Garvin et al., Kim et al., and Xie et al. all successfully extracted this parameter from free text documents and described practical extraction techniques \cite{xie2017extracting, garvin2012automated, kim2017extraction}. In \cite{patterson2017unlocking}, a natural language-based method was presented which uses a predefined dictionary, expert rules and predefined patterns to extract echocardiography measurements from documents. In this study, a pattern-matching algorithm was created and tested to extract term candidates from a large set of clinical notes. The presented method relies heavily on pattern matching, but it can also identify possible misspellings and synonyms by iterative extraction. Wells et al. also successfully extracted a set of predefined parameters, including wall thicknesses, chamber dimensions or flow velocities \cite{wells2014extraction}. They applied NLP to parse the most frequently measured dimensions and used outlier analysis to filter out unrealistic values. Toepfer et al. developed and evaluated an information extraction component with fine-grained terminology that enabled them to recognise almost all relevant information stated in German transthoracic echocardiography reports at the University Hospital of Würzburg \cite{toepfer2015fine}. Jonnalagadda et al. described an information extraction-based approach that automatically converts unstructured text into structured data, which is cross-referenced against eligibility criteria using a rule-based system to determine which patients qualify for a heart failure with preserved ejection fraction (HFpEF) clinical trial \cite{jonnalagadda2017text}. In \cite{renganathan2017text}, Renganathan proposed text mining techniques that enable the extraction of unknown knowledge from unstructured documents.

Going beyond the limitations of the proposed methods, I suggested a generally applicable text mining method \cite{szeker2023general} for extracting numerical test results with their descriptions from free-text-written echocardiography reports. The proposed method abandons regex-based information extraction and employs corpus-independent text mining techniques to extract information from medical texts. It automatically detects expressions containing textual descriptions of the test results and pairs them with their numerical measurement results. The identification of candidate terms is performed by using fuzzy matching utilising the Jaro-Winkler distance to match them to standardised clinical terms. For finding the most suitable text similarity measure, I analysed different distance metrics in, namely Longest Common Subsequence (LCS), Levenshtein distance (LD), weighted Levenshtein distance (WLD), Jaro-Winkler distance, and cosine distance. My experimental results showed that the Jaro-Winkler can discover the most candidate terms at a given threshold.

The suggested similarity-based mapping makes it possible to handle typos, synonyms and abbreviations flexibly; therefore, the efficacy of the information extraction is significantly increased. Additionally, the proposed method can extract multiple information from the documents by a single search, and a repetitive scan is not needed. The proposed method is mainly recommended for the rapid processing of large volumes of echocardiographic findings, such as to support medical research or to verify patient selection criteria for clinical trials quickly.

\vspace{1.0cm}

The rest of my thesis is organised as follows. Chapter \ref{chap:control_group_selection} deals with the problems of control group selection, including novel selection and evaluation methods, while Chapter \ref{chap:text_mining} introduces the proposed text mining method to extract information from echocardiography documents.

% The rest of my thesis is organised as follows. Chapter \ref{chap:control_group_selection} introduces the proposed dissimilarity measures and the developed control group selection methods in detail, and proves their effectiveness on different synthetic and real datasets. Chapter \ref{chap:control_group_selection} also contains information about the effect of missing covariates on the outcome variable. Chapter \ref{chap:text_mining} introduces the proposed text mining method and presents the usability of the method on a Hungarian corpus. Chapter \ref{chap:text_mining} also contains a comparison of different text similarity measures. The proposed text mining method utilises the findings of this comparison.
